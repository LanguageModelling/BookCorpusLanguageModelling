{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobson.n/.conda/envs/bookcorpus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Data.Library import Library\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, latent_size=8, in_channels = 1, out_channels=10, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "         # Use hippo matrices for A and B\n",
    "        self.A = self.gen_A(self.latent_size)\n",
    "        self.B = self.gen_B(self.latent_size)\n",
    "        self.C = torch.rand(self.latent_size, self.out_channels).to(self.device).requires_grad_()\n",
    "        nn.init.xavier_uniform_(self.C)\n",
    "\n",
    "        self.D = torch.rand(self.in_channels, self.out_channels).to(self.device).requires_grad_()\n",
    "        nn.init.xavier_uniform_(self.D)\n",
    "        self.K = None\n",
    "        self.A_stack = None\n",
    "        self.B_stack = None\n",
    "\n",
    "    def gen_A(self, N):\n",
    "        A = torch.zeros(N, N)\n",
    "        for n in range(N):\n",
    "            for k in range(N):\n",
    "                if n > k:\n",
    "                    A[n, k] = -(2*n + 1)**.5 * (2*k + 1)**.5\n",
    "                elif n == k:\n",
    "                    A[n, k] = -(n+1)\n",
    "        return A\n",
    "        \n",
    "    def gen_B(self, N):\n",
    "        B = torch.zeros(N)\n",
    "        for n in range(N):\n",
    "            B[n] = (2*n+1)**.5\n",
    "        return nn.Parameter(B.unsqueeze(1)).to(self.device)\n",
    "        \n",
    "    def discretize(self, step):\n",
    "        A = self.A.to('cpu')\n",
    "        B = self.B.to('cpu')\n",
    "        N = A.shape[0]\n",
    "        I = torch.eye(N)\n",
    "        A_bar = torch.linalg.solve_triangular(I - (step / 2.0) * A, (I + (step / 2.0) * A), upper=False)\n",
    "        B_bar = torch.linalg.solve_triangular((I - (step / 2.0) * A), B * step, upper=False)\n",
    "        return A_bar.to(self.device), B_bar.to(self.device)\n",
    "    \n",
    "    def get_legendre_kernel(self, seq_length):\n",
    "        if self.A_stack is None or self.B_stack is None:\n",
    "            self.A_stack = torch.eye(self.latent_size).unsqueeze(-1).repeat((1, 1, seq_length)).permute(2, 0, 1).to(self.device)\n",
    "            self.B_stack = torch.zeros(seq_length, self.latent_size, self.in_channels).to(self.device)\n",
    "            for idx in range(0, seq_length):\n",
    "                A_bar, B_bar = self.discretize(step=1.0/(idx + 1))\n",
    "                self.A_stack[:idx] = self.A_stack[:idx] @ A_bar\n",
    "                self.B_stack[idx] = B_bar\n",
    "            self.A_stack = self.A_stack.permute(1, 2, 0).cpu()\n",
    "            self.B_Stack = self.B_stack.cpu()\n",
    "            return self.A_stack, self.B_stack\n",
    "        else:\n",
    "            if seq_length > self.A_stack.shape[2]:\n",
    "                print('recalcing kernel')\n",
    "                self.A_stack = None\n",
    "                self.B_stack = None\n",
    "                return self.get_legendre_kernel(seq_length)\n",
    "            else:\n",
    "                return self.A_stack, self.B_stack\n",
    "    def forward(self, u, conv=True):\n",
    "        # u has shape [batch_size, in_channels, sequence_length]\n",
    "        if len(u.shape) == 2:\n",
    "            u = u.unsqueeze(1)\n",
    "        elif len(u.shape) != 3:\n",
    "            print('Unknown number of dimensions in forward_conv')\n",
    "            assert False\n",
    "        batch_size, in_channels, seq_length = u.shape\n",
    "        if conv:\n",
    "            output = self.get_legendre_conv(u.to(self.device))\n",
    "        else:\n",
    "            output = self.get_legendre_rec(u.to(self.device))\n",
    "        output = output.detach()\n",
    "        output =  output @ self.C + u.permute(2, 0, 1) @ self.D\n",
    "        return output.permute(1, 2, 0)\n",
    "    \n",
    "    def get_legendre_rec(self, sequence):\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        x = torch.zeros([batch_size, self.latent_size, seq_length + 1]).to(self.device)\n",
    "        for idx in range(seq_length):\n",
    "            A_bar, B_bar = self.discretize(step=1.0/(1+idx))\n",
    "            x[:, :, idx+1] = x[:,:, idx] @ A_bar.mT + sequence[:,:,idx] @ B_bar.mT\n",
    "        # x has shape [batch_size, self.latent_size, seq_length + 1]\n",
    "        output = x[:, :, 1:]\n",
    "        # Output is of shape [batch_size, latent_size, seq_length]\n",
    "        return (self.B * output).permute(2, 0, 1)\n",
    "    def get_legendre_conv(self, sequence):\n",
    "        # Sequence of shape [batch_size, in_channels, seq_length]\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        A_stack, B_stack = self.get_legendre_kernel(seq_length)\n",
    "        # Apply B stack\n",
    "        u = torch.bmm(sequence.permute(2, 0, 1), B_stack.to(self.device).mT).permute(1, 2, 0)\n",
    "        # u has shape [batch_size, latent_size, seq_length]\n",
    "        # Pad front with zeros to create correct convolutional form\n",
    "        u_pad = F.pad(u, (seq_length-1, 0))\n",
    "        # u_pad has shape [batch_size, latent_size, 2*seq_length-1]\n",
    "        output = F.conv1d(u_pad, A_stack.to(self.device))\n",
    "        # Output is of shape [batch_size, latent_size, seq_length]\n",
    "        return (self.B * output).permute(2, 0, 1)\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_internal, device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.device = device\n",
    "        self.log_softmax = nn.LogSoftmax(-2)\n",
    "\n",
    "        # Define model shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.S4 = S4Layer(latent_size = self.d_internal, in_channels = self.d_model, out_channels = self.vocab_size, device=self.device)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        batch_size, seq_length = sequence.shape\n",
    "        x = self.embeddings(sequence)\n",
    "        # Current shape is [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1).to(self.device)\n",
    "         # Current shape is [batch_size, channels, sequence_length]\n",
    "        x = self.S4(x)\n",
    "        x = self.log_softmax(x)\n",
    "        x = x.to(torch.device('cpu')) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0:Total Loss:686.59:Perplexity:4015.70\n",
      "1:Total Loss:345.05:Perplexity:66.24\n",
      "2:Total Loss:197.54:Perplexity:20.06\n",
      "3:Total Loss:162.22:Perplexity:14.94\n",
      "4:Total Loss:151.50:Perplexity:13.22\n",
      "5:Total Loss:146.44:Perplexity:12.37\n",
      "6:Total Loss:143.48:Perplexity:11.87\n",
      "7:Total Loss:141.54:Perplexity:11.54\n",
      "8:Total Loss:140.18:Perplexity:11.32\n",
      "9:Total Loss:139.18:Perplexity:11.15\n",
      "10:Total Loss:138.42:Perplexity:11.03\n",
      "11:Total Loss:137.82:Perplexity:10.93\n",
      "12:Total Loss:137.34:Perplexity:10.85\n",
      "13:Total Loss:136.93:Perplexity:10.79\n",
      "14:Total Loss:136.60:Perplexity:10.73\n",
      "15:Total Loss:136.31:Perplexity:10.69\n",
      "16:Total Loss:136.07:Perplexity:10.65\n",
      "17:Total Loss:135.87:Perplexity:10.62\n",
      "18:Total Loss:135.70:Perplexity:10.59\n",
      "19:Total Loss:135.55:Perplexity:10.57\n",
      "20:Total Loss:135.42:Perplexity:10.54\n",
      "21:Total Loss:135.31:Perplexity:10.53\n",
      "22:Total Loss:135.21:Perplexity:10.51\n",
      "23:Total Loss:135.12:Perplexity:10.49\n",
      "24:Total Loss:135.04:Perplexity:10.48\n",
      "25:Total Loss:134.96:Perplexity:10.47\n",
      "26:Total Loss:134.90:Perplexity:10.46\n",
      "27:Total Loss:134.84:Perplexity:10.45\n",
      "28:Total Loss:134.78:Perplexity:10.44\n",
      "29:Total Loss:134.73:Perplexity:10.43\n",
      "30:Total Loss:134.69:Perplexity:10.42\n",
      "31:Total Loss:134.65:Perplexity:10.42\n",
      "32:Total Loss:134.61:Perplexity:10.41\n",
      "33:Total Loss:134.57:Perplexity:10.40\n",
      "34:Total Loss:134.54:Perplexity:10.40\n",
      "35:Total Loss:134.51:Perplexity:10.39\n",
      "36:Total Loss:134.48:Perplexity:10.39\n",
      "37:Total Loss:134.45:Perplexity:10.38\n",
      "38:Total Loss:134.43:Perplexity:10.38\n",
      "39:Total Loss:134.40:Perplexity:10.37\n",
      "40:Total Loss:134.38:Perplexity:10.37\n",
      "41:Total Loss:134.36:Perplexity:10.37\n",
      "42:Total Loss:134.34:Perplexity:10.36\n",
      "43:Total Loss:134.32:Perplexity:10.36\n",
      "44:Total Loss:134.31:Perplexity:10.36\n",
      "45:Total Loss:134.29:Perplexity:10.35\n",
      "46:Total Loss:134.27:Perplexity:10.35\n",
      "47:Total Loss:134.26:Perplexity:10.35\n",
      "48:Total Loss:134.25:Perplexity:10.35\n",
      "49:Total Loss:134.23:Perplexity:10.34\n",
      "50:Total Loss:134.22:Perplexity:10.34\n",
      "51:Total Loss:134.21:Perplexity:10.34\n",
      "52:Total Loss:134.20:Perplexity:10.34\n",
      "53:Total Loss:134.18:Perplexity:10.34\n",
      "54:Total Loss:134.17:Perplexity:10.33\n",
      "55:Total Loss:134.16:Perplexity:10.33\n",
      "56:Total Loss:134.15:Perplexity:10.33\n",
      "57:Total Loss:134.15:Perplexity:10.33\n",
      "58:Total Loss:134.14:Perplexity:10.33\n",
      "59:Total Loss:134.13:Perplexity:10.33\n",
      "60:Total Loss:134.12:Perplexity:10.32\n",
      "61:Total Loss:134.11:Perplexity:10.32\n",
      "62:Total Loss:134.10:Perplexity:10.32\n",
      "63:Total Loss:134.10:Perplexity:10.32\n",
      "64:Total Loss:134.09:Perplexity:10.32\n",
      "65:Total Loss:134.08:Perplexity:10.32\n",
      "66:Total Loss:134.08:Perplexity:10.32\n",
      "67:Total Loss:134.07:Perplexity:10.32\n",
      "68:Total Loss:134.06:Perplexity:10.31\n",
      "69:Total Loss:134.06:Perplexity:10.31\n",
      "70:Total Loss:134.05:Perplexity:10.31\n",
      "71:Total Loss:134.05:Perplexity:10.31\n",
      "72:Total Loss:134.04:Perplexity:10.31\n",
      "73:Total Loss:134.04:Perplexity:10.31\n",
      "74:Total Loss:134.03:Perplexity:10.31\n",
      "75:Total Loss:134.03:Perplexity:10.31\n",
      "76:Total Loss:134.02:Perplexity:10.31\n",
      "77:Total Loss:134.02:Perplexity:10.31\n",
      "78:Total Loss:134.02:Perplexity:10.31\n",
      "79:Total Loss:134.01:Perplexity:10.31\n",
      "80:Total Loss:134.01:Perplexity:10.31\n",
      "81:Total Loss:134.00:Perplexity:10.30\n",
      "82:Total Loss:134.00:Perplexity:10.30\n",
      "83:Total Loss:134.00:Perplexity:10.30\n",
      "84:Total Loss:133.99:Perplexity:10.30\n",
      "85:Total Loss:133.99:Perplexity:10.30\n",
      "86:Total Loss:133.99:Perplexity:10.30\n",
      "87:Total Loss:133.99:Perplexity:10.30\n",
      "88:Total Loss:133.98:Perplexity:10.30\n",
      "89:Total Loss:133.98:Perplexity:10.30\n",
      "90:Total Loss:133.98:Perplexity:10.30\n",
      "91:Total Loss:133.98:Perplexity:10.30\n",
      "92:Total Loss:133.97:Perplexity:10.30\n",
      "93:Total Loss:133.97:Perplexity:10.30\n",
      "94:Total Loss:133.97:Perplexity:10.30\n",
      "95:Total Loss:133.97:Perplexity:10.30\n",
      "96:Total Loss:133.96:Perplexity:10.30\n",
      "97:Total Loss:133.96:Perplexity:10.30\n",
      "98:Total Loss:133.96:Perplexity:10.30\n",
      "99:Total Loss:133.96:Perplexity:10.30\n",
      "100:Total Loss:133.96:Perplexity:10.30\n",
      "101:Total Loss:133.95:Perplexity:10.30\n",
      "102:Total Loss:133.95:Perplexity:10.30\n",
      "103:Total Loss:133.95:Perplexity:10.30\n",
      "104:Total Loss:133.95:Perplexity:10.30\n",
      "105:Total Loss:133.95:Perplexity:10.30\n",
      "106:Total Loss:133.95:Perplexity:10.30\n",
      "107:Total Loss:133.94:Perplexity:10.30\n",
      "108:Total Loss:133.94:Perplexity:10.29\n",
      "109:Total Loss:133.94:Perplexity:10.29\n",
      "110:Total Loss:133.94:Perplexity:10.29\n",
      "111:Total Loss:133.94:Perplexity:10.29\n",
      "112:Total Loss:133.94:Perplexity:10.29\n",
      "113:Total Loss:133.93:Perplexity:10.29\n",
      "114:Total Loss:133.93:Perplexity:10.29\n",
      "115:Total Loss:133.93:Perplexity:10.29\n",
      "116:Total Loss:133.93:Perplexity:10.29\n",
      "117:Total Loss:133.93:Perplexity:10.29\n",
      "118:Total Loss:133.93:Perplexity:10.29\n",
      "119:Total Loss:133.93:Perplexity:10.29\n",
      "120:Total Loss:133.93:Perplexity:10.29\n",
      "121:384:6.8239\r"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "epochs = 128\n",
    "lr = .001\n",
    "seq_length=512\n",
    "batch_size=128\n",
    "d_model=64\n",
    "d_internal=64\n",
    "train_size = 2**16\n",
    "encoding=76\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda')\n",
    "library = Library(encoding = encoding, train_size = train_size, streaming=False)\n",
    "\n",
    "model = S4Model(encoding, d_model, d_internal, device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam([model.S4.C, model.S4.D], lr=lr)\n",
    "x_batch = torch.zeros([batch_size, seq_length-1])\n",
    "y_batch = torch.zeros([batch_size, seq_length-1])\n",
    "losses = torch.zeros(epochs)\n",
    "perplexities = torch.zeros(epochs)\n",
    "print('Training')\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    dataloader = library.get_train_dataloader(seq_length)\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        mod_idx = idx % batch_size\n",
    "        if data.shape[0] != seq_length:\n",
    "            break # End of usable dataloader\n",
    "        x_batch[mod_idx] = data[:-1]\n",
    "        y_batch[mod_idx] = data[1:]\n",
    "        if mod_idx == batch_size-1:\n",
    "            # Update weights\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(x_batch.long())\n",
    "            loss = loss_fn(y_pred, y_batch.long())\n",
    "            losses[epoch] += loss\n",
    "\n",
    "            print(f'{epoch}:{idx+1}:{losses[epoch]:.4f}', end='\\r')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    # Test\n",
    "    perplexities[epoch] = library.calc_perplexity(model)\n",
    "    print(f'{epoch}:Total Loss:{losses[epoch]:.2f}:Perplexity:{perplexities[epoch]:.2f}')\n",
    "    torch.save(model.state_dict(), f'Models/{encoding}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookcorpus",
   "language": "python",
   "name": "bookcorpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
