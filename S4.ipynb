{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobson.n/.conda/envs/bookcorpus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Data.Library import Library\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (538603807.py, line 119)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 119\u001b[0;36m\u001b[0m\n\u001b[0;31m    x = self.embeddings(sequence.to(self.device)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, latent_size=8, in_channels = 1, out_channels=10, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "         # Use hippo matrices for A and B\n",
    "        self.A = self.gen_A(self.latent_size)\n",
    "        self.B = self.gen_B(self.latent_size)\n",
    "        self.C = torch.rand(self.latent_size, self.out_channels).to(self.device).requires_grad_()\n",
    "        nn.init.xavier_uniform_(self.C)\n",
    "\n",
    "        self.D = torch.rand(self.in_channels, self.out_channels).to(self.device).requires_grad_()\n",
    "        nn.init.xavier_uniform_(self.D)\n",
    "        self.K = None\n",
    "        self.A_stack = None\n",
    "        self.B_stack = None\n",
    "\n",
    "    def gen_A(self, N):\n",
    "        A = torch.zeros(N, N)\n",
    "        for n in range(N):\n",
    "            for k in range(N):\n",
    "                if n > k:\n",
    "                    A[n, k] = -(2*n + 1)**.5 * (2*k + 1)**.5\n",
    "                elif n == k:\n",
    "                    A[n, k] = -(n+1)\n",
    "        return A\n",
    "        \n",
    "    def gen_B(self, N):\n",
    "        B = torch.zeros(N)\n",
    "        for n in range(N):\n",
    "            B[n] = (2*n+1)**.5\n",
    "        return nn.Parameter(B.unsqueeze(1)).to(self.device)\n",
    "        \n",
    "    def discretize(self, step):\n",
    "        A = self.A.to('cpu')\n",
    "        B = self.B.to('cpu')\n",
    "        N = A.shape[0]\n",
    "        I = torch.eye(N)\n",
    "        A_bar = torch.linalg.solve_triangular(I - (step / 2.0) * A, (I + (step / 2.0) * A), upper=False)\n",
    "        B_bar = torch.linalg.solve_triangular((I - (step / 2.0) * A), B * step, upper=False)\n",
    "        return A_bar.to(self.device), B_bar.to(self.device)\n",
    "    \n",
    "    def get_legendre_kernel(self, seq_length):\n",
    "        if self.A_stack is None or self.B_stack is None:\n",
    "            self.A_stack = torch.eye(self.latent_size).unsqueeze(-1).repeat((1, 1, seq_length)).permute(2, 0, 1).to(self.device)\n",
    "            self.B_stack = torch.zeros(seq_length, self.latent_size, self.in_channels).to(self.device)\n",
    "            for idx in range(0, seq_length):\n",
    "                A_bar, B_bar = self.discretize(step=1.0/(idx + 1))\n",
    "                self.A_stack[:idx] = self.A_stack[:idx] @ A_bar\n",
    "                self.B_stack[idx] = B_bar\n",
    "            self.A_stack = self.A_stack.permute(1, 2, 0).cpu()\n",
    "            self.B_Stack = self.B_stack.cpu()\n",
    "            return self.A_stack, self.B_stack\n",
    "        else:\n",
    "            if seq_length > self.A_stack.shape[2]:\n",
    "                print('recalcing kernel')\n",
    "                self.A_stack = None\n",
    "                self.B_stack = None\n",
    "                return self.get_legendre_kernel(seq_length)\n",
    "            else:\n",
    "                return self.A_stack, self.B_stack\n",
    "    def forward(self, u, conv=True):\n",
    "        # u has shape [batch_size, in_channels, sequence_length]\n",
    "        if len(u.shape) == 2:\n",
    "            u = u.unsqueeze(1)\n",
    "        elif len(u.shape) != 3:\n",
    "            print('Unknown number of dimensions in forward_conv')\n",
    "            assert False\n",
    "        batch_size, in_channels, seq_length = u.shape\n",
    "        if conv:\n",
    "            output = self.get_legendre_conv(u.to(self.device))\n",
    "        else:\n",
    "            output = self.get_legendre_rec(u.to(self.device))\n",
    "        output = output.detach()\n",
    "        output =  output @ self.C + u.permute(2, 0, 1) @ self.D\n",
    "        return output.permute(1, 2, 0)\n",
    "    \n",
    "    def get_legendre_rec(self, sequence):\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        x = torch.zeros([batch_size, self.latent_size, seq_length + 1]).to(self.device)\n",
    "        for idx in range(seq_length):\n",
    "            A_bar, B_bar = self.discretize(step=1.0/(1+idx))\n",
    "            x[:, :, idx+1] = x[:,:, idx] @ A_bar.mT + sequence[:,:,idx] @ B_bar.mT\n",
    "        # x has shape [batch_size, self.latent_size, seq_length + 1]\n",
    "        output = x[:, :, 1:]\n",
    "        # Output is of shape [batch_size, latent_size, seq_length]\n",
    "        return (self.B * output).permute(2, 0, 1)\n",
    "    def get_legendre_conv(self, sequence):\n",
    "        # Sequence of shape [batch_size, in_channels, seq_length]\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        A_stack, B_stack = self.get_legendre_kernel(seq_length)\n",
    "        # Apply B stack\n",
    "        u = torch.bmm(sequence.permute(2, 0, 1), B_stack.to(self.device).mT).permute(1, 2, 0)\n",
    "        # u has shape [batch_size, latent_size, seq_length]\n",
    "        # Pad front with zeros to create correct convolutional form\n",
    "        u_pad = F.pad(u, (seq_length-1, 0))\n",
    "        # u_pad has shape [batch_size, latent_size, 2*seq_length-1]\n",
    "        output = F.conv1d(u_pad, A_stack.to(self.device))\n",
    "        # Output is of shape [batch_size, latent_size, seq_length]\n",
    "        return (self.B * output).permute(2, 0, 1)\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_internal, device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.device = device\n",
    "        self.log_softmax = nn.LogSoftmax(-2)\n",
    "\n",
    "        # Define model shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model).to(self.device)\n",
    "        self.S4 = S4Layer(latent_size = self.d_internal, in_channels = self.d_model, out_channels = self.vocab_size, device=self.device)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        batch_size, seq_length = sequence.shape\n",
    "        x = self.embeddings(sequence.to(self.device)\n",
    "        # Current shape is [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)#.to(self.device)\n",
    "         # Current shape is [batch_size, channels, sequence_length]\n",
    "        x = self.S4(x)\n",
    "        x = self.log_softmax(x)\n",
    "        x = x.to(torch.device('cpu')) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "epochs = 64\n",
    "lr = .001\n",
    "seq_length=512\n",
    "batch_size=64\n",
    "d_model=128\n",
    "d_internal=128\n",
    "train_size = 2**16\n",
    "encoding='50k'\n",
    "vocab_size = 50256\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda')\n",
    "library = Library(encoding = encoding, train_size = train_size, streaming=False)\n",
    "\n",
    "model = S4Model(vocab_size, d_model, d_internal, device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam([model.S4.C, model.S4.D], lr=lr)\n",
    "x_batch = torch.zeros([batch_size, seq_length-1])\n",
    "y_batch = torch.zeros([batch_size, seq_length-1])\n",
    "losses = torch.zeros(epochs)\n",
    "perplexities = torch.zeros(epochs)\n",
    "print('Training')\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    dataloader = library.get_train_dataloader(seq_length)\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        mod_idx = idx % batch_size\n",
    "        if data.shape[0] != seq_length:\n",
    "            break # End of usable dataloader\n",
    "        x_batch[mod_idx] = data[:-1]\n",
    "        y_batch[mod_idx] = data[1:]\n",
    "        if mod_idx == batch_size-1:\n",
    "            # Update weights\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(x_batch.long())\n",
    "            loss = loss_fn(y_pred, y_batch.long())\n",
    "            losses[epoch] += loss\n",
    "\n",
    "            print(f'{epoch}:{idx+1}:{losses[epoch]:.4f}', end='\\r')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    # Test\n",
    "    perplexities[epoch] = library.calc_perplexity(model)\n",
    "    print(f'{epoch}:Total Loss:{losses[epoch]:.2f}:Perplexity:{perplexities[epoch]:.2f}')\n",
    "    torch.save(model.state_dict(), f'Models/{encoding}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookcorpus",
   "language": "python",
   "name": "bookcorpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
