{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Data.Library import Library\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_internal, num_layers, device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.log_softmax = nn.LogSoftmax(-2)\n",
    "\n",
    "        # Define model shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model).to(self.device)\n",
    "        self.S4_layers = nn.ModuleList()\n",
    "        self.num_params = self.vocab_size*self.d_model\n",
    "        for layer in range(self.num_layers):\n",
    "            self.S4_layers.append(S4Layer(latent_size = self.d_internal, in_channels = self.d_model, out_channels = self.d_model, device=self.device))\n",
    "            self.num_params += self.S4_layers[-1].num_params\n",
    "        self.fc1 = nn.Linear(self.d_model, self.vocab_size)\n",
    "    def forward(self, sequence, conv=True):\n",
    "        batch_size, seq_length = sequence.shape\n",
    "        x = self.embeddings(sequence.to(self.device))\n",
    "        # Current shape is [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)#.to(self.device)\n",
    "         # Current shape is [batch_size, channels, sequence_length]\n",
    "        for layer in self.S4_layers:\n",
    "            x = self.S4(x, conv=conv)\n",
    "            x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.log_softmax(x)\n",
    "        x = x.to(torch.device('cpu')) \n",
    "        return x\n",
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, latent_size=8, in_channels = 1, out_channels=10, device=torch.device('cpu'), max_seq_length=1000):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "         # Use hippo matrices for A and B\n",
    "        self.A = self.gen_A(self.latent_size)\n",
    "        self.B = self.gen_B(self.latent_size)\n",
    "        self.C = nn.Linear(self.in_channels*self.latent_size, self.out_channels).to(self.device)\n",
    "        self.D = nn.Linear(self.in_channels, self.out_channels).to(self.device)\n",
    "        self.A_stack = None\n",
    "        self.B_stack = None\n",
    "        self.log_softmax = nn.LogSoftmax(-1)\n",
    "        self.num_params = self.in_channels*(self.latent_size + 1)*self.out_channels\n",
    "\n",
    "    def gen_A(self, N):\n",
    "        A = torch.zeros(N, N)\n",
    "        for n in range(N):\n",
    "            for k in range(N):\n",
    "                if n > k:\n",
    "                    A[n, k] = -(2*n + 1)**.5 * (2*k + 1)**.5\n",
    "                elif n == k:\n",
    "                    A[n, k] = -(n+1)\n",
    "        return A\n",
    "        \n",
    "    def gen_B(self, N):\n",
    "        B = torch.zeros(N)\n",
    "        for n in range(N):\n",
    "            B[n] = (2*n+1)**.5\n",
    "        return B.unsqueeze(1)\n",
    "        \n",
    "    def discretize(self, step):\n",
    "        A = self.A.to('cpu')\n",
    "        B = self.B.to('cpu')\n",
    "        N = A.shape[0]\n",
    "        I = torch.eye(N)\n",
    "        A_bar = torch.linalg.solve_triangular(I - (step / 2.0) * A, (I + (step / 2.0) * A), upper=False)\n",
    "        B_bar = torch.linalg.solve_triangular((I - (step / 2.0) * A), B * step, upper=False)\n",
    "        return A_bar.to(self.device), B_bar.to(self.device)\n",
    "    \n",
    "    def get_legendre_kernel(self, seq_length):\n",
    "        if self.A_stack is None or self.B_stack is None:\n",
    "            A_stack = torch.eye(self.latent_size).unsqueeze(-1).repeat((1, 1, seq_length)).permute(2, 0, 1).to(self.device)\n",
    "            A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "            for idx in range(0, seq_length):\n",
    "                A_stack[:idx] = A_stack[:idx] @ A_bar\n",
    "            A_stack = A_stack.permute(1, 2, 0).cpu()\n",
    "            self.A_stack = A_stack\n",
    "            return A_stack\n",
    "        else:\n",
    "            if seq_length > self.A_stack.shape[2]:\n",
    "                print('recalcing kernel')\n",
    "                self.A_stack = None\n",
    "                return self.get_legendre_kernel(seq_length)\n",
    "            else:\n",
    "                return self.A_stack\n",
    "            \n",
    "    def get_legendre_rec(self, sequence):\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        x = torch.zeros([batch_size,self.latent_size, in_channels,  seq_length + 1]).to(self.device)\n",
    "        A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "        B_bar = B_bar.permute(1, 0).repeat(batch_size, 1).unsqueeze(2) # Shape of [batch_size, latent_sizem, 1]\n",
    "        A_bar = A_bar.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        for idx in range(seq_length):\n",
    "            seq = sequence[:,:,idx].unsqueeze(1)\n",
    "            B_term = torch.matmul(B_bar, seq)\n",
    "            x_term = x[:,:,:,idx]\n",
    "            A_term = torch.matmul(A_bar, x_term)\n",
    "            x[:, :, :, idx+1] = A_term + B_term\n",
    "        # x has shape [batch_size, self.latent_size, in_channels, seq_length + 1]\n",
    "        output = x[:, :, :, 1:]\n",
    "        # output has shape [batch_size, self.latent_size, in_channels, seq_length]\n",
    "        B_scale = self.B.unsqueeze(0).unsqueeze(2).to(self.device)\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = (B_scale * output).permute(0, 2, 1, 3)\n",
    "        return output\n",
    "    \n",
    "    def get_legendre_conv(self, sequence):\n",
    "        # Sequence of shape [batch_size, in_channels, seq_length]\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        A_stack = self.get_legendre_kernel(seq_length)\n",
    "        A_bar, B_bar = self.discretize(step = 1.0/seq_length)\n",
    "        # Apply B\n",
    "        u = sequence\n",
    "        u = B_bar.unsqueeze(0).unsqueeze(2) * u.unsqueeze(1)\n",
    "        # Convolution\n",
    "        K = A_stack.unsqueeze(2).to(self.device)\n",
    "        output = F.conv2d(u, K, padding=(0, seq_length-1))[:, :, :, :seq_length]\n",
    "        # output has shape [batch_size, self.latent_size, in_channels, seq_length]\n",
    "        B_scale = self.B.unsqueeze(0).unsqueeze(2).to(self.device)\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = (B_scale * output).permute(0, 2, 1, 3)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, u, conv=True):\n",
    "        # u has shape [batch_size, in_channels, sequence_length]\n",
    "        if len(u.shape) == 2: # If 1 dimensional\n",
    "            u = u.unsqueeze(1)\n",
    "        if conv:\n",
    "            output = self.get_legendre_conv(u.to(self.device))\n",
    "        else:\n",
    "            output = self.get_legendre_rec(u.to(self.device))\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        # output permuted to [batch_size, seq_length, in_channels, latent_size]\n",
    "        # Flatten feature space to [batch_size, seq_length, in_channels * latent_size]\n",
    "        output = output.permute(0, 3, 1, 2).flatten(2)\n",
    "        output =  self.C(output) + self.D(u.permute(0, 2, 1))\n",
    "        # output should now be of shape [batch_size, seq_length, out channels]\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2432\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m S4Model(vocab_size, d_model, d_internal, num_layers, device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mnum_params)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[1;32m     23\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "epochs = 64\n",
    "lr = .001\n",
    "seq_length=256\n",
    "batch_size=64\n",
    "d_model=32\n",
    "d_internal=128\n",
    "num_layers=1\n",
    "train_size = 2**20\n",
    "test_size = 2**16\n",
    "encoding=76\n",
    "vocab_size = 76\n",
    "torch.manual_seed(0)\n",
    "conv = True\n",
    "\n",
    "# Setup\n",
    "device = torch.device('mps')\n",
    "#library = Library(encoding=encoding, train_size=train_size, test_size=test_size, download_new=True)\n",
    "model = S4Model(vocab_size, d_model, d_internal, num_layers, device)\n",
    "print(model.num_params)\n",
    "assert False\n",
    "loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "x_batch = torch.zeros([batch_size, seq_length-1])\n",
    "y_batch = torch.zeros([batch_size, seq_length-1])\n",
    "losses = torch.zeros(epochs)\n",
    "perplexities = torch.zeros(epochs)\n",
    "print('Training')\n",
    "import time\n",
    "tic = time.time()\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    dataloader = library.get_train_dataloader(seq_length)\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        mod_idx = idx % batch_size\n",
    "        if data.shape[0] != seq_length:\n",
    "            break # End of usable dataloader\n",
    "        x_batch[mod_idx] = data[:-1]\n",
    "        y_batch[mod_idx] = data[1:]\n",
    "        if mod_idx == batch_size-1:\n",
    "            # Update weights\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(x_batch.long(), conv=conv)\n",
    "            loss = loss_fn(y_pred, y_batch.long())\n",
    "            losses[epoch] += loss\n",
    "\n",
    "            print(f'{epoch}:{idx+1}:{losses[epoch]:.4f}', end='\\r')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    # Test\n",
    "    perplexities[epoch] = library.calc_perplexity(model, batch_size=64, seq_length=256)\n",
    "    print(f'{epoch}:Total Loss:{losses[epoch]:.2f}:Perplexity:{perplexities[epoch]:.2f}')\n",
    "    torch.save(model.state_dict(), f'Models/{encoding}.pkl')\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookcorpus",
   "language": "python",
   "name": "bookcorpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
