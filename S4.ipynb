{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobson.n/.conda/envs/bookcorpus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Data.Library import Library\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_internal, num_layers, device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.log_softmax = nn.LogSoftmax(-2)\n",
    "\n",
    "        # Define model shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model).to(self.device)\n",
    "        self.S4_layers = nn.ModuleList()\n",
    "        self.num_params = self.vocab_size*self.d_model\n",
    "        for layer in range(self.num_layers):\n",
    "            self.S4_layers.append(S4Layer(latent_size = self.d_internal, in_channels = self.d_model, out_channels = self.d_model, device=self.device))\n",
    "            self.num_params += self.S4_layers[-1].num_params\n",
    "        self.fc1 = nn.Linear(self.d_model, self.vocab_size).to(self.device)\n",
    "    def forward(self, sequence, conv=True):\n",
    "        batch_size, seq_length = sequence.shape\n",
    "        x = self.embeddings(sequence.to(self.device))\n",
    "        # Current shape is [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)#.to(self.device)\n",
    "         # Current shape is [batch_size, channels, sequence_length]\n",
    "        for layer in self.S4_layers:\n",
    "            x = layer(x, conv=conv)\n",
    "            x = F.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.log_softmax(x)\n",
    "        x = x.to(torch.device('cpu')) \n",
    "        return x\n",
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, latent_size=8, in_channels = 1, out_channels=10, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "         # Use hippo matrices for A and B\n",
    "        self.A = self.gen_A(self.latent_size)\n",
    "        self.B = self.gen_B(self.latent_size)\n",
    "        self.C = nn.Linear(self.in_channels*self.latent_size, self.out_channels).to(self.device)\n",
    "        self.D = nn.Linear(self.in_channels, self.out_channels).to(self.device)\n",
    "        self.A_stack = None\n",
    "        self.step = None\n",
    "        self.A_bar = None\n",
    "        self.B_bar = None\n",
    "        self.log_softmax = nn.LogSoftmax(-1)\n",
    "        self.num_params = self.in_channels*(self.latent_size + 1)*self.out_channels + self.latent_size**2 + self.latent_size # Last two terms include hippo matrices\n",
    "\n",
    "    def gen_A(self, N):\n",
    "        A = torch.zeros(N, N)\n",
    "        for n in range(N):\n",
    "            for k in range(N):\n",
    "                if n > k:\n",
    "                    A[n, k] = -(2*n + 1)**.5 * (2*k + 1)**.5\n",
    "                elif n == k:\n",
    "                    A[n, k] = -(n+1)\n",
    "        return A\n",
    "        \n",
    "    def gen_B(self, N):\n",
    "        B = torch.zeros(N)\n",
    "        for n in range(N):\n",
    "            B[n] = (2*n+1)**.5\n",
    "        return B.unsqueeze(1)\n",
    "        \n",
    "    def discretize(self, step):\n",
    "        if self.step == step and self.B_bar is not None and self.A_bar is not None:\n",
    "            return self.A_bar, self.B_bar\n",
    "        else:\n",
    "            A = self.A.to('cpu')\n",
    "            B = self.B.to('cpu')\n",
    "            N = A.shape[0]\n",
    "            I = torch.eye(N)\n",
    "            A_bar = torch.linalg.solve_triangular(I - (step / 2.0) * A, (I + (step / 2.0) * A), upper=False).to(self.device)\n",
    "            B_bar = torch.linalg.solve_triangular((I - (step / 2.0) * A), B * step, upper=False).to(self.device)\n",
    "            self.A_bar = A_bar\n",
    "            self.B_bar = B_bar                                                                                                        \n",
    "            return self.A_bar, self.B_bar\n",
    "    \n",
    "    def get_legendre_kernel(self, seq_length):\n",
    "        if self.A_stack is None:\n",
    "            A_stack = torch.eye(self.latent_size).unsqueeze(-1).repeat((1, 1, seq_length)).permute(2, 0, 1).to(self.device)\n",
    "            A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "            for idx in range(0, seq_length):\n",
    "                A_stack[:idx] = A_stack[:idx] @ A_bar\n",
    "            A_stack = A_stack.permute(1, 2, 0).cpu()\n",
    "            self.A_stack = A_stack\n",
    "            return A_stack\n",
    "        else:\n",
    "            if seq_length > self.A_stack.shape[2]:\n",
    "                print('recalcing kernel')\n",
    "                self.A_stack = None\n",
    "                return self.get_legendre_kernel(seq_length)\n",
    "            else:\n",
    "                return self.A_stack\n",
    "            \n",
    "    def get_legendre_rec(self, sequence):\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        x = torch.zeros([batch_size,self.latent_size, in_channels,  seq_length + 1]).to(self.device)\n",
    "        A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "        B_bar = B_bar.permute(1, 0).repeat(batch_size, 1).unsqueeze(2) # Shape of [batch_size, latent_sizem, 1]\n",
    "        A_bar = A_bar.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        for idx in range(seq_length):\n",
    "            seq = sequence[:,:,idx].unsqueeze(1)\n",
    "            B_term = torch.matmul(B_bar, seq)\n",
    "            x_term = x[:,:,:,idx]\n",
    "            A_term = torch.matmul(A_bar, x_term)\n",
    "            x[:, :, :, idx+1] = A_term + B_term\n",
    "        # x has shape [batch_size, self.latent_size, in_channels, seq_length + 1]\n",
    "        output = x[:, :, :, 1:]\n",
    "        # output has shape [batch_size, self.latent_size, in_channels, seq_length]\n",
    "        B_scale = self.B.unsqueeze(0).unsqueeze(2).to(self.device)\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = (B_scale * output).permute(0, 2, 1, 3)\n",
    "        return output\n",
    "    \n",
    "    def get_legendre_conv(self, sequence):\n",
    "        # Sequence of shape [batch_size, in_channels, seq_length]\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        A_stack = self.get_legendre_kernel(seq_length)\n",
    "        A_bar, B_bar = self.discretize(step = 1.0/seq_length)\n",
    "        # Apply B\n",
    "        u = sequence\n",
    "        u = B_bar.unsqueeze(0).unsqueeze(2) * u.unsqueeze(1)\n",
    "        # Convolution\n",
    "        K = A_stack.unsqueeze(2).to(self.device)\n",
    "        output = F.conv2d(u, K, padding=(0, seq_length-1))[:, :, :, :seq_length]\n",
    "        # output has shape [batch_size, self.latent_size, in_channels, seq_length]\n",
    "        B_scale = self.B.unsqueeze(0).unsqueeze(2).to(self.device)\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = (B_scale * output).permute(0, 2, 1, 3)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, u, conv=True):\n",
    "        # u has shape [batch_size, in_channels, sequence_length]\n",
    "        if len(u.shape) == 2: # If 1 dimensional\n",
    "            u = u.unsqueeze(1)\n",
    "        if conv:\n",
    "            output = self.get_legendre_conv(u.to(self.device))\n",
    "        else:\n",
    "            output = self.get_legendre_rec(u.to(self.device))\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        # output permuted to [batch_size, seq_length, in_channels, latent_size]\n",
    "        # Flatten feature space to [batch_size, seq_length, in_channels * latent_size]\n",
    "        output = output.permute(0, 3, 1, 2).flatten(2)\n",
    "        output =  self.C(output) + self.D(u.permute(0, 2, 1))\n",
    "        # output should now be of shape [batch_size, seq_length, out channels]\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S4 Model with 8562688 parameters\n",
      "Training\n",
      "0:448:26.4844\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod_idx \u001b[38;5;241m==\u001b[39m batch_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     44\u001b[0m     losses[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/.conda/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mS4Model.forward\u001b[0;34m(self, sequence, conv)\u001b[0m\n\u001b[1;32m     24\u001b[0m  \u001b[38;5;66;03m# Current shape is [batch_size, channels, sequence_length]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS4_layers:\n\u001b[0;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 143\u001b[0m, in \u001b[0;36mS4Layer.forward\u001b[0;34m(self, u, conv)\u001b[0m\n\u001b[1;32m    141\u001b[0m     u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conv:\n\u001b[0;32m--> 143\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_legendre_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_legendre_rec(u\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "Cell \u001b[0;32mIn[4], line 133\u001b[0m, in \u001b[0;36mS4Layer.get_legendre_conv\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m    131\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(u, K, padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, seq_length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[:, :, :, :seq_length]\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# output has shape [batch_size, self.latent_size, in_channels, seq_length]\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m B_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# output has shape [batch_size, in_channels, latent_size, sequence_length]\u001b[39;00m\n\u001b[1;32m    135\u001b[0m output \u001b[38;5;241m=\u001b[39m (B_scale \u001b[38;5;241m*\u001b[39m output)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "epochs = 64\n",
    "lr = .001\n",
    "seq_length=256\n",
    "batch_size=64\n",
    "d_model=128\n",
    "d_internal=192\n",
    "num_layers=2\n",
    "train_size = 2**20\n",
    "test_size = 2**16\n",
    "encoding=76\n",
    "vocab_size = 76\n",
    "torch.manual_seed(0)\n",
    "conv = True\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda')\n",
    "library = Library(encoding=encoding, train_size=train_size, test_size=test_size, download_new=False)\n",
    "model = S4Model(vocab_size, d_model, d_internal, num_layers, device)\n",
    "print(f'S4 Model with {model.num_params} parameters')\n",
    "loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "x_batch = torch.zeros([batch_size, seq_length-1])\n",
    "y_batch = torch.zeros([batch_size, seq_length-1])\n",
    "losses = torch.zeros(epochs)\n",
    "perplexities = torch.zeros(epochs)\n",
    "print('Training')\n",
    "import time\n",
    "tic = time.time()\n",
    "# Training\n",
    "dataloader = library.get_train_dataloader(seq_length)\n",
    "for epoch in range(epochs):\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        mod_idx = idx % batch_size\n",
    "        if data.shape[0] != seq_length:\n",
    "            break # End of usable dataloader\n",
    "        x_batch[mod_idx] = data[:-1]\n",
    "        y_batch[mod_idx] = data[1:]\n",
    "        if mod_idx == batch_size-1:\n",
    "            # Update weights\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(x_batch.long(), conv=conv)\n",
    "            loss = loss_fn(y_pred, y_batch.long())\n",
    "            losses[epoch] += loss\n",
    "\n",
    "            print(f'{epoch}:{idx+1}:{losses[epoch]:.4f}', end='\\r')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    # Test\n",
    "    perplexities[epoch] = library.calc_perplexity(model, batch_size=64, seq_length=256)\n",
    "    print(f'{epoch}:Total Loss:{losses[epoch]:.2f}:Perplexity:{perplexities[epoch]:.2f}')\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(perplexities, 'S411Kperplexities.pt')\n",
    "torch.save(model.state_dict(), f'Models/S411K.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122971/3668444734.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('S411Kperplexities.pt')[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.7838)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('S411Kperplexities.pt')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookcorpus",
   "language": "python",
   "name": "bookcorpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
