{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobson.n/.conda/envs/bookcorpus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from Data.Library import Library\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [255, 128] but got: [255, 64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[423], line 137\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    136\u001b[0m model \u001b[38;5;241m=\u001b[39m S4Model(vocab_size, d_model, d_internal, device)\n\u001b[0;32m--> 137\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[423], line 132\u001b[0m, in \u001b[0;36mS4Model.forward\u001b[0;34m(self, sequence, conv)\u001b[0m\n\u001b[1;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.to(self.device)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m  \u001b[38;5;66;03m# Current shape is [batch_size, channels, sequence_length]\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(x)\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[423], line 69\u001b[0m, in \u001b[0;36mS4Layer.forward\u001b[0;34m(self, u, conv)\u001b[0m\n\u001b[1;32m     67\u001b[0m     u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conv:\n\u001b[0;32m---> 69\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_legendre_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_legendre_rec(u\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "Cell \u001b[0;32mIn[423], line 106\u001b[0m, in \u001b[0;36mS4Layer.get_legendre_conv\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m    104\u001b[0m u \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    105\u001b[0m B_stack \u001b[38;5;241m=\u001b[39m B_stack\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 106\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [255, 128] but got: [255, 64]."
     ]
    }
   ],
   "source": [
    "class S4Layer(nn.Module):\n",
    "    def __init__(self, latent_size=8, in_channels = 1, out_channels=10, device=torch.device('cpu'), max_seq_length=1000):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "         # Use hippo matrices for A and B\n",
    "        self.A = self.gen_A(self.latent_size)\n",
    "        self.B = self.gen_B(self.latent_size)\n",
    "        self.C = nn.Linear(self.latent_size, self.out_channels).to(self.device)\n",
    "        self.D = nn.Linear(self.in_channels, self.out_channels).to(self.device)\n",
    "        self.A_stack = None\n",
    "        self.B_stack = None\n",
    "        self.log_softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "    def gen_A(self, N):\n",
    "        A = torch.zeros(N, N)\n",
    "        for n in range(N):\n",
    "            for k in range(N):\n",
    "                if n > k:\n",
    "                    A[n, k] = -(2*n + 1)**.5 * (2*k + 1)**.5\n",
    "                elif n == k:\n",
    "                    A[n, k] = -(n+1)\n",
    "        return A\n",
    "        \n",
    "    def gen_B(self, N):\n",
    "        B = torch.zeros(N)\n",
    "        for n in range(N):\n",
    "            B[n] = (2*n+1)**.5\n",
    "        return B.unsqueeze(1)\n",
    "        \n",
    "    def discretize(self, step):\n",
    "        A = self.A.to('cpu')\n",
    "        B = self.B.to('cpu')\n",
    "        N = A.shape[0]\n",
    "        I = torch.eye(N)\n",
    "        A_bar = torch.linalg.solve_triangular(I - (step / 2.0) * A, (I + (step / 2.0) * A), upper=False)\n",
    "        B_bar = torch.linalg.solve_triangular((I - (step / 2.0) * A), B * step, upper=False)\n",
    "        return A_bar.to(self.device), B_bar.to(self.device)\n",
    "    \n",
    "    def get_legendre_kernel(self, seq_length):\n",
    "        if self.A_stack is None or self.B_stack is None:\n",
    "            A_stack = torch.eye(self.latent_size).unsqueeze(-1).repeat((1, 1, seq_length)).permute(2, 0, 1).to(self.device)\n",
    "            B_stack = torch.zeros(seq_length, self.latent_size, self.in_channels).to(self.device)\n",
    "            A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "            for idx in range(0, seq_length):\n",
    "                A_stack[:idx] = A_stack[:idx] @ A_bar\n",
    "                B_stack[idx] = B_bar\n",
    "            A_stack = A_stack.permute(1, 2, 0).cpu()\n",
    "            self.A_stack = A_stack\n",
    "            self.B_stack = B_stack\n",
    "            return A_stack, B_stack\n",
    "        else:\n",
    "            if seq_length > self.A_stack.shape[2]:\n",
    "                print('recaslcing kernel')\n",
    "                self.A_stack = None\n",
    "                self.B_stack = None\n",
    "                return self.get_legendre_kernel(seq_length)\n",
    "            else:\n",
    "                return self.A_stack, self.B_stack\n",
    "    \n",
    "    def forward(self, u, conv=True):\n",
    "        # u has shape [batch_size, in_channels, sequence_length]\n",
    "        if len(u.shape) == 2: # If 1 dimensional\n",
    "            u = u.unsqueeze(1)\n",
    "        if conv:\n",
    "            output = self.get_legendre_conv(u.to(self.device))\n",
    "        else:\n",
    "            output = self.get_legendre_rec(u.to(self.device))\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = output.detach()\n",
    "        print(output.shape)\n",
    "        assert False\n",
    "        output =  self.C(output) + self.D(u.permute(2, 0, 1))\n",
    "        return output.permute(1, 2, 0)\n",
    "    \n",
    "    def get_legendre_rec(self, sequence):\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        x = torch.zeros([batch_size,self.latent_size, in_channels,  seq_length + 1]).to(self.device)\n",
    "        A_bar, B_bar = self.discretize(step=1.0/seq_length)\n",
    "        B_bar = B_bar.permute(1, 0).repeat(batch_size, 1).unsqueeze(2) # Shape of [batch_size, latent_sizem, 1]\n",
    "        A_bar = A_bar.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        for idx in range(seq_length):\n",
    "            seq = sequence[:,:,idx].unsqueeze(1)\n",
    "            B_term = torch.matmul(B_bar, seq)\n",
    "            x_term = x[:,:,:,idx]\n",
    "            A_term = torch.matmul(A_bar, x_term)\n",
    "            x[:, :, :, idx+1] = A_term + B_term\n",
    "        # x has shape [batch_size, self.latent_size, in_channels, seq_length + 1]\n",
    "        output = x[:, :, :, 1:]\n",
    "        # output has shape [batch_size, self.latent_size, in_channels, seq_length]\n",
    "        B_scale = self.B.unsqueeze(0).unsqueeze(2).to(self.device)\n",
    "        # output has shape [batch_size, in_channels, latent_size, sequence_length]\n",
    "        output = (B_scale * output).permute(0, 2, 1, 3)\n",
    "        return output\n",
    "    \n",
    "    def get_legendre_conv(self, sequence):\n",
    "        # Sequence of shape [batch_size, in_channels, seq_length]\n",
    "        batch_size, in_channels, seq_length = sequence.shape\n",
    "        A_stack, B_stack = self.get_legendre_kernel(seq_length)\n",
    "        # Apply B stack\n",
    "        u = sequence.permute(2,0,1)\n",
    "        print(u.shape)\n",
    "        assert False\n",
    "        B_stack = B_stack.permute(0, 2, 1).to(self.device)\n",
    "        u = torch.matmul(B_stack, u)\n",
    "        print(u.shape)\n",
    "        u = u.permute(1, 2, 0)\n",
    "        # u has shape [batch_size, latent_size, seq_length]\n",
    "        output = F.conv1d(u, A_stack.to(self.device), padding=seq_length-1)[:, :, :seq_length]\n",
    "        # Output is of shape [batch_size, latent_size, seq_length]\n",
    "        return (self.B * output).permute(2, 0, 1)\n",
    "class S4Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_internal, device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.device = device\n",
    "        self.log_softmax = nn.LogSoftmax(-2)\n",
    "\n",
    "        # Define model shape\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model).to(self.device)\n",
    "        self.S4 = S4Layer(latent_size = self.d_internal, in_channels = self.d_model, out_channels = self.vocab_size, device=self.device)\n",
    "    \n",
    "    def forward(self, sequence, conv=True):\n",
    "        batch_size, seq_length = sequence.shape\n",
    "        x = self.embeddings(sequence.to(self.device))\n",
    "        # Current shape is [batch_size, sequence_length, channels]\n",
    "        x = x.permute(0, 2, 1)#.to(self.device)\n",
    "         # Current shape is [batch_size, channels, sequence_length]\n",
    "        x = self.S4(x, conv=conv)\n",
    "        x = self.log_softmax(x)\n",
    "        x = x.to(torch.device('cpu')) \n",
    "        return x\n",
    "model = S4Model(vocab_size, d_model, d_internal, device)\n",
    "model(x_batch.long(), conv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [255, 1] but got: [255, 32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[378], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod_idx \u001b[38;5;241m==\u001b[39m batch_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     44\u001b[0m     losses[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[377], line 126\u001b[0m, in \u001b[0;36mS4Model.forward\u001b[0;34m(self, sequence, conv)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.to(self.device)\u001b[39;00m\n\u001b[1;32m    125\u001b[0m  \u001b[38;5;66;03m# Current shape is [batch_size, channels, sequence_length]\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(x)\n\u001b[1;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/bookcorpus/lib/python3.10/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[377], line 100\u001b[0m, in \u001b[0;36mS4Layer.forward\u001b[0;34m(self, u, conv)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conv:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(in_channels):\n\u001b[0;32m--> 100\u001b[0m         output[:, i, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_legendre_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(in_channels):\n",
      "Cell \u001b[0;32mIn[377], line 82\u001b[0m, in \u001b[0;36mS4Layer.get_legendre_conv\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     80\u001b[0m u \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m B_stack \u001b[38;5;241m=\u001b[39m B_stack\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 82\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_stack\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# u has shape [batch_size, latent_size, seq_length]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [255, 1] but got: [255, 32]."
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "epochs = 64\n",
    "lr = .001\n",
    "seq_length=256\n",
    "batch_size=64\n",
    "d_model=32\n",
    "d_internal=128\n",
    "train_size = 2**20\n",
    "test_size = 2**16\n",
    "encoding=76\n",
    "vocab_size = 76\n",
    "torch.manual_seed(0)\n",
    "conv = True\n",
    "\n",
    "# Setup\n",
    "device = torch.device('mps')\n",
    "if False:\n",
    "    library = Library(encoding=encoding, train_size=train_size, test_size=test_size, download_new=True)\n",
    "\n",
    "model = S4Model(vocab_size, d_model, d_internal, device)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "x_batch = torch.zeros([batch_size, seq_length-1])\n",
    "y_batch = torch.zeros([batch_size, seq_length-1])\n",
    "losses = torch.zeros(epochs)\n",
    "perplexities = torch.zeros(epochs)\n",
    "print('Training')\n",
    "import time\n",
    "tic = time.time()\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    dataloader = library.get_train_dataloader(seq_length)\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        mod_idx = idx % batch_size\n",
    "        if data.shape[0] != seq_length:\n",
    "            break # End of usable dataloader\n",
    "        x_batch[mod_idx] = data[:-1]\n",
    "        y_batch[mod_idx] = data[1:]\n",
    "        if mod_idx == batch_size-1:\n",
    "            # Update weights\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(x_batch.long(), conv=conv)\n",
    "            loss = loss_fn(y_pred, y_batch.long())\n",
    "            losses[epoch] += loss\n",
    "\n",
    "            print(f'{epoch}:{idx+1}:{losses[epoch]:.4f}', end='\\r')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "    # Test\n",
    "    perplexities[epoch] = library.calc_perplexity(model, batch_size=64, seq_length=256)\n",
    "    print(f'{epoch}:Total Loss:{losses[epoch]:.2f}:Perplexity:{perplexities[epoch]:.2f}')\n",
    "    torch.save(model.state_dict(), f'Models/{encoding}.pkl')\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bookcorpus",
   "language": "python",
   "name": "bookcorpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
